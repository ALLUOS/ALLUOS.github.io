---
layout: post
title: User Testing
description: Learn more about the project's testing strategy and outlook from the second semester.
---
### Introduction

**Software testing** is the process of analyzing a software tool and related documentation to identify defects and improve product quality. Due to the fact that software testing becomes part of the programming stage, developers have the opportunity to fix bugs already at the initial stage of development. This reduces the risk of defects in the finished product. It is important to find bugs or weaknesses at the initial level, and the earlier the process begins, the easier it will be to make changes to the overall structure of the application.
Continuing the work of the testing group, we followed a similar strategy. But with some differences. In this semester we divided our strategy into two main dimensions: **Technical Testing** and **User Acceptance Testing**.

### Methods: Technical Testing
Technical testing of this and last semester followed a test case-based testing strategy - a formalized approach in which testing is performed based on pre-prepared test cases, test case sets, and other documentation. This method also allows you to achieve maximum completeness of the application research due to the strict systematization of the process. A well-written test case allows you to store information for long-term use and exchange of experience between testers and teams. Thanks to this, we were able to complete unfinished test cases from the last semester without any further questions. This proves once again the importance of correct technical documentation.

As mentioned above, the term "test case" can refer to the formal recording of a test case in the form of a technical document. This record has a generally accepted structure, the components of which are called test case attributes. Below is an example of one of the test cases we wrote (Pic.1)

![image](https://user-images.githubusercontent.com/57707401/111073452-06b48780-84df-11eb-886a-c956751c1bb2.png)

Picture 1 "Test Case Collection" example.

Along with the tests that were written in the last semester, but were not conducted, we wrote new test cases that relate to the new features of the application: 1) a new adaptive module 2) achievements 3) a third task "Discussion". The new third task developed this semester was tested only at the User Acceptance Testing phase, because the preliminary version of the assignment was made only by the end of the semester. The third task requires the creation of new test cases and the corresponding testing.
Testing of new application functions was carried out on specially created special branches of the code (discussion_task branch), which, after a certain number of tests, will be added to the main application code.

With testing on the Telegram API, there is always a question of which accounts to use for the quality of experiments. For this, as in the last semester, our course was provided with special testing Telegram accounts. All necessary data can be obtained from course tutors or responsible testers.

### Methods: User Acceptance Testing

**Acceptance Testing** - formalized testing aimed at checking the application from the point of view of the end-user and making a decision on whether the customer accepts the work from the project team.

In this section, we will explain the general principles of this type of testing and the following strategy. In the next sections, you can find more accurate data about the group of end-users (testing participants) and results.

The first stage of testing is a formal explanation to the group of participants the purposes of this testing, especially explaining to them that it is not their knowledge that is being tested, but the quality of our product. Along with this, the subjects are given minimal instructions for how to interact with the bot for the first time. The second important step is to collect information about users so that it might be possible to correctly organize groups by interest, age, and level of knowledge (Pic.2). This information also provides importance for future data analyzes.

![image](https://user-images.githubusercontent.com/57707401/111086708-f4a60980-851d-11eb-8194-cf6cd0fb2ec2.png)

Picture 2 Pre-Test Questionnaire

This testing also carried another function - to check whether the application helps to improve their language skills with consistent use. Initially, the testing requirement was a daily interaction of at least 30 minutes for 2 weeks. However, missed meetings were also allowed, but no more than 2 times a week. After the completion of two weeks, participants must pass a post-test survey to assess the quality of the product and the experience gained, where they can also leave their feedback on any of the parties to the project (Pic.3).

![image](https://user-images.githubusercontent.com/57707401/111086946-3daa8d80-851f-11eb-8ff0-f21e17a2312a.png)

Picture 3 Post-Test Questionnaire

The final step is to pass the so-called quiz, specially compiled on the logs of their answers and the answers of other participants (Pic.4).

![image](https://user-images.githubusercontent.com/57707401/111087031-b7db1200-851f-11eb-8974-4de478d7f7e5.png)

Picture 4 "Proficiency Quiz" Example


##### Personalized Learning Outcome Post-Tests

To gauge if the application has affected individual user's language skills we employ a learning outcome post-test strategy involving personalized langauge skill assessments. As mentioned in the [Adaptive Data]() section, the application periodically collects various user performance metadata which are used both to adjust application difficulty and to inform the aforementioned learning outcome post-test. Given the application's adaptive learning strategy, it is difficult to design a post-test of language skill which can assumedly be compared across all participants in a controlled manner. In light of this, we use the performance metadata to automatically generate personalized skill assessments which aim to, for each participant, test whether items which proved particularly challenging during application use can be successfully answered.

For example, the [sentence correction]() task is scored according to the number of task iteration phases which are successfully passed. This allows us to rank users' task iterations by their retrospective challenge, assuming that task iterations with lower scores were more challenging for a student. We then present these task items to the student during the learning outcome post-test. Likewise, we also present customized post-test tasks for the [vocabulary guessing]() task, incorporating user-generated descriptions to achieve a matching task following the methodology of [#]. The image below shows an example of such generated vocabulary matching questions.

<img class="center" src="{{ '' | absolute_url }}/assets/images/vocab-quiz-generated.png">

### Results: User Acceptance Testing

The below sections document the analysis and results from the aforementioned user acceptance tests.

#### Test Group Descriptive Statistics

We were starting off with eight students in our testing group, but three of them left after a short amount of time. The five remaining participants were all second language learners. The age of the students was very homogenous, one participant was 13 years old while the all other ones were 14. Accordingly, they were visiting either the seventh or eight grade in school. They all defined their language level as B1- intermediate and therefore met the lower boundaries of our target group, which we defined as either B1 or B2 learners.
In terms of relevant applications, only one actually had experience with what we would generally define as learning apps, namely Doulingo, Lingualeo and Skyeng. One other user already used a dictionary website as learning support. Regarding language exchange applications, only one of the students already experienced this type of language learning. All participants apart from one already had experience with Telegram, which is fortunate as this is the basis of our application and would therefore allow low barriers. Luckily, all of the users answered that they were interested in using an English learning app for collaborative studying.

#### Quantitative Data Analysis

Although test group size and testing phase duration are limited so as to constrain the power of any qualitative analysis, we can still derive insights which facilitate an improved understanding of the user-application interaction and suggest avenues for further development which will likely improve the application's usability.

Superficial investigation of the backend proficiency values used to attune task item selection to student performance validate the adaptive module's approach to gradually adjust task item difficulty. Given that proficiency values increase and decrease relative to student performance, we can observe these values to bootstrap an inference regarding the learning outcomes of the test group and the operation of the adaptive module.  Looking at the overall distribution of final test-period proficiency values irrespective of sub-type, we find a trend towards values greater than 5. Given that all proficiency values are initialized at 5 and increase or decrease depending on performance, this result validates the operation of the adaptive module when cross-referenced with the moving average of the presented task item difficulties (shown below) over the testing period, which shows a positive trend towards higher difficulty (R2 0.48).

<img class="center" style="width: 60%" src="{{ '' | absolute_url }}/assets/images/proficiency value density.png">

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab difficulty.png">

Additionally, given that we track [user performance]() vis-à-vis various task-specific data points, by visualizing these performance metrics over time we can intuit the behavior of the [adaptive module]() relative to user performance. Figure [#] shows [sentence correction]() task data for one user during the test period, with each unique data point representing a single task iteration, where task items sharing a grammatical sub-type are connected by color-coded lines.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/task duration over time.png">

This graph makes clear the relative duration of task items over time for this student, demonstrating that many task item sub-types, for example, sub-type 12 (subjunctive, pink line) first require significantly more time than by the end of the testing phase, suggesting that the user was able to improve their performance with this sub-type as measured by task duration. This single-user case study also suggests an avenue for further development, namely that a user be able to focus on a subset of sub-types in order to demonstrate proficiency improvements before moving onto other types of task items. While this user was presented with a variety of task sub-types via the adaptive module, allowing the user to selectively focus on only a few sub-types may improve learning outcomes for selected sub-types.

Regarding the [vocabulary guessing]() task, by visualizing the number of messages sent by non-elected users (i.e., the number of guesses) for words of each task iteration, we can identify those words which proved particularly-challenging for this test group to correctly guess or explain.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab words num messages.png">

Nevertheless, it may be the case that words which required relatively more guesses do not correlate with our difficulty assumptions informing the [adaptive module](), therefore compromising the application's user model and implying that additional attention to user performance may be valuable to properly attune task selection to user proficiency. When visualizing expected word difficulty as determined by [neural network classification]() against number of guesses, we find that many words of expectedly low difficulty required relatively more guesses, and vice-versa. Given that the number of guesses is not incorporated into the adaptive module computations, which presently only consider the correct/incorrect status of a given task iteration, these data suggest that the adaptive module's ability to suggest appropriately-challenging task items could be improved by integrating the number of guesses as a relevant data source.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab words difficulty.png">

Of final note are the aforementioned learning outcome post-tests, which are bespoke to each student based on previously-missed items. For a set of post-tests comprised of 20 questions each (10 sentence correction, 10 vocabulary matching), the median error rate is 1/20, ranging from 1/20 to 0/20. Although extremely limited, this result suggests that participants could make some learning progress relative to questions which were previously-challenging. It may be that the functionality for retrieving previous questions is more valuable to application features emphasizing spaced-repetition learning, which suggest one avenue for future development.

For additional visualizations and data not mentioned in this section, please refer to the [resources]() below.

#### Qualitative Data Analysis
We asked the learners to provide us some feedback on their experience with the application overall as well as with the individual tasks. Therefore we gave them the opportunity to fill out a questionnaire regarding their usage of the application.

The first task, concerning sentence correction, was overall rated at a 6.75/10 by the users. While they liked the idea of the task, they would have wished the examples to be more fitting to what they already learned in school, for example more tasks concerning verb forms. When they hadn’t encountered a certain grammatical form in school yet, it seemed to be challenging to even complete this task. But apart from that, the learning progress they achieved by mastering the complex task was mentioned as a positive point.

The second task, vocabulary guessing was rated at a 10/10. Accordingly, the feedback was exclusively positive. The focus group enjoyed the experienced autonomy and collaboration as well as the learning of new words. While the learners marked the task being quite easy as a positive point, it might be a take-away to enhance the difficulty to improve the learning process.

The third task, discussion, was rated at a 6.5/10 by the learners. The feedback we gained was mostly focused on the high difficulty of the task for the learners and the need for confidence to participate in the discussion. Apart from that, they appreciated the opportunity to strengthen their language proficiency of this task as it was, in their opinion, the most challenging one.

We specifically asked the users for their feedback on the newly displayed achievements, since this was a new feature we added this semester. The overall response was very positive, with the users seeing this as a fun feature to keep track of their achieved goals. On the other hand, one user felt like the propose of the achievements was not entirely clear to them. This might result from the times of testing being some days apart, so that the users were not able to actually develop a streak. Nonetheless it might be interesting to keep in mind.

The overall feedback from the users was in general very positive. The students did  not only seem to enjoy the usage of the application, but also saw potential in future developments.

Based on these useful remark, we can take multiple ideas into the future development of the application. First off, we should consider to adapt the difficulty of the tasks more to the actual proficiency of the specific user, as the vocabulary guessing seemed to be quite easy while the other two seemed quite challenging. Additionally it might be important to include more explanation on the achievement module.

### Conclusions
Although our evaluations remain limited by the total number of participants, we were able to expand upon our previous semester's test methodologies with more quantitative data sources reflecting user performance, while also receiving additional valuable feedback which will help to improve the application. Notably, it is clear that further attention to application difficulty is needed, particularly the apparent simplicity of the vocabulary guessing task. Some application bugs still persist which can disrupt user experience. Finally, investigation of user performance presents several avenues for future feature development.

### Recommendations for Coming Semester
From the above analyses and discussion we derive several recommendations valuable to coming project iterations:

- Allow users to focus on a subset of task sub-types by improving the persistence of the path selection module.
- Integrate the number of vocabulary guessing task iterations guesses as a relevant variable used to adapt task difficulty.
- Repurpose the automated post-test generation logic for additional application features involving spaced-repetition.
- Attend to the difficulty of the sentence correction and vocabulary guessing task items to handle a broader range of user proficiencies.

### Resources
[Final Test Proficiency Distribution for Each Subtype]({{ '' | absolute_url }}/assets/images/hist.zip")

### References
[#] Vesselinov, R., & Grego, J. (2012). Duolingo effectiveness study. *City University of New York, USA,* 1-25
