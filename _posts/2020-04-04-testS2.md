---
layout: post
title: Evaluation Semester Two
description: Learn more about the project's testing strategy and outlook from the second semester.
---
### Methods: Technical Testing
[Ivan]

### Methods: User Acceptance Testing
[Ivan]

##### Personalized Learning Outcome Post-Tests

To gauge if the application has affected individual user's language skills we employ a learning outcome post-test strategy involving personalized langauge skill assessments. As mentioned in the [Adaptive Data]() section, the application periodically collects various user performance metadata which are used both to adjust application difficulty and to inform the aforementioned learning outcome post-test. Given the application's adaptive learning strategy, it is difficult to design a post-test of language skill which can assumedly be compared across all participants in a controlled manner. In light of this, we use the performance metadata to automatically generate personalized skill assessments which aim to, for each participant, test whether items which proved particularly challenging during application use can be successfully answered.

For example, the [sentence correction]() task is scored according to the number of task iteration phases which are successfully passed. This allows us to rank users' task iterations by their retrospective challenge, assuming that task iterations with lower scores were more challenging for a student. We then present these task items to the student during the learning outcome post-test. Likewise, we also present customized post-test tasks for the [vocabulary guessing]() task, incorporating user-generated descriptions to achieve a matching task following the methodology of [#]. The image below shows an example of such generated vocabulary matching questions.

<img class="center" src="{{ '' | absolute_url }}/assets/images/vocab-quiz-generated.png">

### Results: User Acceptance Testing

The below sections document the analysis and results from the aforementioned user acceptance tests.

#### Test Group Descriptive Statistics

We were starting off with eight students in our testing group, but three of them left after a short amount of time. The five remaining participants were all second language learners. The age of the students was very homogenous, one participant was 13 years old while the all other ones were 14. Accordingly, they were visiting either the seventh or eight grade in school. They all defined their language level as B1- intermediate and therefore met the lower boundaries of our target group, which we defined as either B1 or B2 learners. 
In terms of relevant applications, only one actually had experience with what we would generally define as learning apps, namely Doulingo, Lingualeo and Skyeng. One other user already used a dictionary website as learning support. Regarding language exchange applications, only one of the students already experienced this type of language learning. All participants apart from one already had experience with Telegram, which is fortunate as this is the basis of our application and would therefore allow low barriers. Luckily, all of the users answered that they were interested in using an English learning app for collaborative studying. 

#### Quantitative Data Analysis

Although test group size and testing phase duration are limited so as to constrain the power of any qualitative analysis, we can still derive insights which facilitate an improved understanding of the user-application interaction and suggest avenues for further development which will likely improve the application's usability.

Superficial investigation of the backend proficiency values used to attune task item selection to student performance validate the adaptive module's approach to gradually adjust task item difficulty. Given that proficiency values increase and decrease relative to student performance, we can observe these values to bootstrap an inference regarding the learning outcomes of the test group and the operation of the adaptive module.  Looking at the overall distribution of final test-period proficiency values irrespective of sub-type, we find a trend towards values greater than 5. Given that all proficiency values are initialized at 5 and increase or decrease depending on performance, this result validates the operation of the adaptive module when cross-referenced with the moving average of the presented task item difficulties (shown below) over the testing period, which shows a positive trend towards higher difficulty (R2 0.48).

<img class="center" style="width: 60%" src="{{ '' | absolute_url }}/assets/images/proficiency value density.png">

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab difficulty.png">

Additionally, given that we track [user performance]() vis-à-vis various task-specific data points, by visualizing these performance metrics over time we can intuit the behavior of the [adaptive module]() relative to user performance. Figure [#] shows [sentence correction]() task data for one user during the test period, with each unique data point representing a single task iteration, where task items sharing a grammatical sub-type are connected by color-coded lines.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/task duration over time.png">

This graph makes clear the relative duration of task items over time for this student, demonstrating that many task item sub-types, for example, sub-type 12 (subjunctive, pink line) first require significantly more time than by the end of the testing phase, suggesting that the user was able to improve their performance with this sub-type as measured by task duration. This single-user case study also suggests an avenue for further development, namely that a user be able to focus on a subset of sub-types in order to demonstrate proficiency improvements before moving onto other types of task items. While this user was presented with a variety of task sub-types via the adaptive module, allowing the user to selectively focus on only a few sub-types may improve learning outcomes for selected sub-types.

Regarding the [vocabulary guessing]() task, by visualizing the number of messages sent by non-elected users (i.e., the number of guesses) for words of each task iteration, we can identify those words which proved particularly-challenging for this test group to correctly guess or explain.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab words num messages.png">

Nevertheless, it may be the case that words which required relatively more guesses do not correlate with our difficulty assumptions informing the [adaptive module](), therefore compromising the application's user model and implying that additional attention to user performance may be valuable to properly attune task selection to user proficiency. When visualizing expected word difficulty as determined by [neural network classification]() against number of guesses, we find that many words of expectedly low difficulty required relatively more guesses, and vice-versa. Given that the number of guesses is not incorporated into the adaptive module computations, which presently only consider the correct/incorrect status of a given task iteration, these data suggest that the adaptive module's ability to suggest appropriately-challenging task items could be improved by integrating the number of guesses as a relevant data source.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab words difficulty.png">

Of final note are the aforementioned learning outcome post-tests, which are bespoke to each student based on previously-missed items. For a set of post-tests comprised of 20 questions each (10 sentence correction, 10 vocabulary matching), the median error rate is 1/20, ranging from 1/20 to 0/20. Although extremely limited, this result suggests that participants could make some learning progress relative to questions which were previously-challenging. It may be that the functionality for retrieving previous questions is more valuable to application features emphasizing spaced-repetition learning, which suggest one avenue for future development.

For additional visualizations and data not mentioned in this section, please refer to the [resources]() below.

#### Qualitative Data Analysis
We asked the learners to provide us some feedback on their experience with the application overall as well as with the individual tasks. Therefore we gave them the opportunity to fill out a questionnaire regarding their usage of the application. 

The first task, concerning sentence correction, was overall rated at a 6.75/10 by the users. While they liked the idea of the task, they would have wished the examples to be more fitting to what they already learned in school, for example more tasks concerning verb forms. When they hadn’t encountered a certain grammatical form in school yet, it seemed to be challenging to even complete this task. But apart from that, the learning progress they achieved by mastering the complex task was mentioned as a positive point. 

The second task, vocabulary guessing was rated at a 10/10. Accordingly, the feedback was exclusively positive. The focus group enjoyed the experienced autonomy and collaboration as well as the learning of new words. While the learners marked the task being quite easy as a positive point, it might be a take-away to enhance the difficulty to improve the learning process. 

The third task, discussion, was rated at a 6.5/10 by the learners. The feedback we gained was mostly focused on the high difficulty of the task for the learners and the need for confidence to participate in the discussion. Apart from that, they appreciated the opportunity to strengthen their language proficiency of this task as it was, in their opinion, the most challenging one. 

We specifically asked the users for their feedback on the newly displayed achievements, since this was a new feature we added this semester. The overall response was very positive, with the users seeing this as a fun feature to keep track of their achieved goals. On the other hand, one user felt like the propose of the achievements was not entirely clear to them. This might result from the times of testing being some days apart, so that the users were not able to actually develop a streak. Nonetheless it might be interesting to keep in mind. 

The overall feedback from the users was in general very positive. The students did  not only seem to enjoy the usage of the application, but also saw potential in future developments. 

Based on these useful remark, we can take multiple ideas into the future development of the application. First off, we should consider to adapt the difficulty of the tasks more to the actual proficiency of the specific user, as the vocabulary guessing seemed to be quite easy while the other two seemed quite challenging. Additionally it might be important to include more explanation on the achievement module. 

### Conclusions
Although our evaluations remain limited by the total number of participants, we were able to expand upon our previous semester's test methodologies with more quantitative data sources reflecting user performance, while also receiving additional valuable feedback which will help to improve the application. Notably, it is clear that further attention to application difficulty is needed, particularly the apparent simplicity of the vocabulary guessing task. Some application bugs still persist which can disrupt user experience. Finally, investigation of user performance presents several avenues for future feature development.

### Recommendations for Coming Semester
From the above analyses and discussion we derive several recommendations valuable to coming project iterations:

- Allow users to focus on a subset of task sub-types by improving the persistence of the path selection module.
- Integrate the number of vocabulary guessing task iterations guesses as a relevant variable used to adapt task difficulty.
- Repurpose the automated post-test generation logic for additional application features involving spaced-repetition.
- Attend to the difficulty of the sentence correction and vocabulary guessing task items to handle a broader range of user proficiencies.

### Resources
[Final Test Proficiency Distribution for Each Subtype]({{ '' | absolute_url }}/assets/images/hist.zip")

### References
[#] Vesselinov, R., & Grego, J. (2012). Duolingo effectiveness study. *City University of New York, USA,* 1-25
