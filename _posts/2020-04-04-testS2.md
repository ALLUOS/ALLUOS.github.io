---
layout: post
title: Evaluation Semester Two
description: Learn more about the project's testing strategy and outlook from the second semester.
---
### Methods: Technical Testing
[Ivan]

### Methods: User Acceptance Testing
[Ivan]

##### Personalized Learning Outcome Post-Tests

To gauge if the application has affected individual user's language skills we employ a learning outcome post-test strategy involving personalized langauge skill assessments. As mentioned in the [Adaptive Data]() section, the application periodically collects various user performance metadata which are used both to adjust application difficulty and to inform the aforementioned learning outcome post-test. Given the application's adaptive learning strategy, it is difficult to design a post-test of language skill which can assumedly be compared across all participants in a controlled manner. In light of this, we use the performance metadata to automatically generate personalized skill assessments which aim to, for each participant, test whether items which proved particularly challenging during application use can be successfully answered.

For example, the [sentence correction]() task is scored according to the number of task iteration phases which are successfully passed. This allows us to rank users' task iterations by their retrospective challenge, assuming that task iterations with lower scores were more challenging for a student. We then present these task items to the student during the learning outcome post-test. Likewise, we also present customized post-test tasks for the [vocabulary guessing]() task, incorporating user-generated descriptions to achieve a matching task following the methodology of [#]. The image below shows an example of such generated vocabulary matching questions.

<img class="center" src="{{ '' | absolute_url }}/assets/images/vocab-quiz-generated.png">

### Results: User Acceptance Testing

The below sections document the analysis and results from the aforementioned user acceptance tests.

#### Test Group Descriptive Statistics
[?]

#### Quantitative Data Analysis

Although test group size and testing phase duration are limited so as to constrain the power of any qualitative analysis, we can still derive insights which facilitate an improved understanding of the user-application interaction and suggest avenues for further development which will likely improve the application's usability.

Superficial investigation of the backend proficiency values used to attune task item selection to student performance validate the adaptive module's approach to gradually adjust task item difficulty. Given that proficiency values increase and decrease relative to student performance, we can observe these values to bootstrap an inference regarding the learning outcomes of the test group and the operation of the adaptive module.  Looking at the overall distribution of final test-period proficiency values irrespective of sub-type, we find a trend towards values greater than 5. Given that all proficiency values are initialized at 5 and increase or decrease depending on performance, this result validates the operation of the adaptive module when cross-referenced with the moving average of the presented task item difficulties (shown below) over the testing period, which shows a positive trend towards higher difficulty (R2 0.48).

<img class="center" style="width: 60%" src="{{ '' | absolute_url }}/assets/images/proficiency value density.png">

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab difficulty.png">

Additionally, given that we track [user performance]() vis-Ã -vis various task-specific data points, by visualizing these performance metrics over time we can intuit the behavior of the [adaptive module]() relative to user performance. Figure [#] shows [sentence correction]() task data for one user during the test period, with each unique data point representing a single task iteration, where task items sharing a grammatical sub-type are connected by color-coded lines.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/task duration over time.png">

This graph makes clear the relative duration of task items over time for this student, demonstrating that many task item sub-types, for example, sub-type 12 (subjunctive, pink line) first require significantly more time than by the end of the testing phase, suggesting that the user was able to improve their performance with this sub-type as measured by task duration. This single-user case study also suggests an avenue for further development, namely that a user be able to focus on a subset of sub-types in order to demonstrate proficiency improvements before moving onto other types of task items. While this user was presented with a variety of task sub-types via the adaptive module, allowing the user to selectively focus on only a few sub-types may improve learning outcomes for selected sub-types.

Regarding the [vocabulary guessing]() task, by visualizing the number of messages sent by non-elected users (i.e., the number of guesses) for words of each task iteration, we can identify those words which proved particularly-challenging for this test group to correctly guess or explain.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab words num messages.png">

Nevertheless, it may be the case that words which required relatively more guesses do not correlate with our difficulty assumptions informing the [adaptive module](), therefore compromising the application's user model and implying that additional attention to user performance may be valuable to properly attune task selection to user proficiency. When visualizing expected word difficulty as determined by [neural network classification]() against number of guesses, we find that many words of expectedly low difficulty required relatively more guesses, and vice-versa. Given that the number of guesses is not incorporated into the adaptive module computations, which presently only consider the correct/incorrect status of a given task iteration, these data suggest that the adaptive module's ability to suggest appropriately-challenging task items could be improved by integrating the number of guesses as a relevant data source.

<img class="center" style="width: 80%" src="{{ '' | absolute_url }}/assets/images/vocab words difficulty.png">

For additional visualizations and data not mentioned in this section, please refer to the [resources]() below.

#### Qualitative Data Analysis
[Joana]

### Conclusions
[?]

### Recommendations for Coming Semester
From the above analyses and discussion we derive several recommendations valuable to coming project iterations:

- Allow users to focus on a subset of task sub-types by improving the persistence of the path selection module.
- Integrate the number of vocabulary guessing task iterations guesses as a relevant variable used to adapt task difficulty.

### Resources
[Final Test Proficiency Distribution for Each Subtype]({{ '' | absolute_url }}/assets/images/hist.zip")

### References
[#] Vesselinov, R., & Grego, J. (2012). Duolingo effectiveness study. *City University of New York, USA,* 1-25
